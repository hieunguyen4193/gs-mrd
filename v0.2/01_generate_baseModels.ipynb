{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034f9139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.decomposition import NMF\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import os\n",
    "import ot\n",
    "import pickle\n",
    "import argparse\n",
    "import Levenshtein\n",
    "import itertools\n",
    "from helper_functions import *\n",
    "\n",
    "inputdir = \"/media/hieunguyen/HNHD01/raw_data/MRD_GW_v1_20250318/Metadata_Genome-wide_Version1_07.03.25\"\n",
    "\n",
    "PROJECT = \"gs-mrd\"\n",
    "release_version = \"20250320\"\n",
    "# release_version = \"20250319_checkRun\"    \n",
    "\n",
    "# expected_spec = 0.90\n",
    "# expected_spec = 0.95\n",
    "# expected_spec = 0.98\n",
    "# expected_spec = 0.99\n",
    "expected_spec = 1\n",
    "\n",
    "##### configurations/paths\n",
    "path_to_main_src = \"/media/hieunguyen/HNSD01/src/gs-mrd/v0.2\"\n",
    "path_to_model_files = f\"{path_to_main_src}/model_files/{release_version}/SPEC_{expected_spec}\"\n",
    "path_to_save_feature_order = f\"{path_to_main_src}/model_files/{release_version}/feature_order\"\n",
    "\n",
    "os.system(f\"mkdir -p {path_to_model_files}\")\n",
    "os.system(f\"mkdir -p {path_to_save_feature_order}\")\n",
    "\n",
    "metadata_not_use = pd.read_excel(os.path.join(inputdir, \"Metadata_Genome-wide Version1_07.03.25.xlsx\"))\n",
    "metadata = pd.read_csv(os.path.join(inputdir, \"meta_full.csv\"))\n",
    "\n",
    "all_train_samples = metadata[metadata[\"Set\"] == \"train\"][\"SampleID\"].unique()\n",
    "all_test_samples = metadata[metadata[\"Set\"] == \"test\"][\"SampleID\"].unique()\n",
    "all_validate_samples = metadata[metadata[\"Set\"] == \"validate\"][\"SampleID\"].unique()\n",
    "\n",
    "##### generate sample list for each class, all samples\n",
    "samplelist = dict()\n",
    "for label in metadata.Cancer.unique():\n",
    "    samplelist[label] = metadata[metadata[\"Cancer\"] == label][\"SampleID\"].to_list()\n",
    "\n",
    "##### generate sample list for each class, train samples only\n",
    "train_samplelist = dict()\n",
    "for label in metadata.Cancer.unique():\n",
    "    train_samplelist[label] = metadata[(metadata[\"Cancer\"] == label) & (metadata[\"Set\"] == \"train\")][\"SampleID\"].to_list()\n",
    "    \n",
    "##### generate sample list for each class, test samples only\n",
    "test_samplelist = dict()\n",
    "for label in metadata.Cancer.unique():\n",
    "    test_samplelist[label] = metadata[(metadata[\"Cancer\"] == label) & (metadata[\"Set\"] == \"test\")][\"SampleID\"].to_list()\n",
    "\n",
    "##### generate sample list for each class, validate samples only\n",
    "validate_samplelist = dict()\n",
    "for label in metadata.Cancer.unique():\n",
    "    validate_samplelist[label] = metadata[(metadata[\"Cancer\"] == label) & (metadata[\"Set\"] == \"validate\")][\"SampleID\"].to_list()\n",
    "\n",
    "##### read features in\n",
    "featuredf = dict()\n",
    "train_featuredf = dict()\n",
    "test_featuredf = dict()\n",
    "validate_featuredf = dict()\n",
    "\n",
    "for f in [\"NUCLEOSOME\", \"FLEN\", \"EM\", \"IchorCNA\"]:\n",
    "    featuredf[f] = pd.read_csv(os.path.join(inputdir, f\"{f}.csv\"))\n",
    "\n",
    "featuredf[\"IchorCNA\"].columns = [\"SampleID\", \"ichorCNA\"]\n",
    "\n",
    "for f in [\"NUCLEOSOME\", \"FLEN\", \"EM\", \"IchorCNA\"]:\n",
    "    train_featuredf[f] = featuredf[f][featuredf[f][\"SampleID\"].isin(all_train_samples)]\n",
    "    test_featuredf[f] = featuredf[f][featuredf[f][\"SampleID\"].isin(all_test_samples)]\n",
    "    validate_featuredf[f] = featuredf[f][featuredf[f][\"SampleID\"].isin(all_validate_samples)]\n",
    "\n",
    "train_featuredf[\"IchorCNA\"].columns = [\"SampleID\", \"ichorCNA\"]\n",
    "test_featuredf[\"IchorCNA\"].columns = [\"SampleID\", \"ichorCNA\"]\n",
    "validate_featuredf[\"IchorCNA\"].columns = [\"SampleID\", \"ichorCNA\"]\n",
    "\n",
    "##### distance matrix based on edit distance of End motif 4bp\n",
    "nucleotides = ['A', 'C', 'G', 'T']\n",
    "motifs = [''.join(p) for p in itertools.product(nucleotides, repeat=4)]\n",
    "\n",
    "# Initialize an empty distance matrix\n",
    "distance_matrix = pd.DataFrame(index=motifs, columns=motifs)\n",
    "\n",
    "# Compute the Levenshtein distance between each pair of 4-mer motifs\n",
    "for motif1 in motifs:\n",
    "    for motif2 in motifs:\n",
    "        distance_matrix.loc[motif1, motif2] = Levenshtein.distance(motif1, motif2)\n",
    "\n",
    "# Convert the distance matrix to integer type\n",
    "M_EM = distance_matrix.to_numpy().copy()\n",
    "M_EM /= M_EM.max() * 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a503a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = dict() \n",
    "\n",
    "for input_feature in [\"EM\", \"FLEN\", \"NUCLEOSOME\"]:\n",
    "    ##### generate average FEATURE in all control samples in this batch\n",
    "    inputdf = train_featuredf[input_feature].copy().set_index(\"SampleID\").T\n",
    "    inputdf[\"Healthy\"] = inputdf[train_samplelist[\"Healthy\"]].mean(axis = 1)\n",
    "    inputdf[[\"Healthy\"]].to_csv(f\"{path_to_model_files}/Healthy_reference_{input_feature}.csv\")\n",
    "    inputdf = inputdf.drop(\"Healthy\", axis = 1)\n",
    "    \n",
    "    ##### calculate OT barycenters\n",
    "    if input_feature == \"EM\":\n",
    "        baryl2 = calculate_barycenter(inputdf = train_featuredf[input_feature].set_index(\"SampleID\").T,\n",
    "                                      samplelist = train_samplelist, \n",
    "                                      n = inputdf.shape[0], show_plot=False, M = M_EM)\n",
    "    else: \n",
    "        baryl2 = calculate_barycenter(inputdf = train_featuredf[input_feature].set_index(\"SampleID\").T,\n",
    "                                      samplelist = train_samplelist, \n",
    "                                      n = inputdf.shape[0], show_plot=False, M = None)\n",
    "    pd.DataFrame(data = baryl2, columns = [\"baryl2\"]).to_csv(f\"{path_to_model_files}/Healthy_OT_{input_feature}_baryl2.csv\", index = False)\n",
    "    \n",
    "    ##### NMF models\n",
    "    X = train_featuredf[input_feature].set_index(\"SampleID\")\n",
    "    model = NMF(n_components=2, init='random', random_state=0, solver = \"mu\")\n",
    "    W = model.fit_transform(X.to_numpy())\n",
    "    H = model.components_\n",
    "    nmfdf = pd.DataFrame(data = W, columns = [\"V1\", \"V2\"])\n",
    "    nmfdf[\"SampleID\"] = list(X.index)\n",
    "    nmfdf[\"V1_scale\"] = nmfdf[[\"V1\", \"V2\"]].apply(lambda x: x[0]/sum(x), axis = 1)\n",
    "    nmfdf[\"V2_scale\"] = nmfdf[[\"V1\", \"V2\"]].apply(lambda x: x[1]/sum(x), axis = 1)\n",
    "    nmfdf = nmfdf.merge(metadata, right_on = \"SampleID\", left_on = \"SampleID\")\n",
    "    sns.lineplot(H[0, ], label = \"Cancer\")\n",
    "    sns.lineplot(H[1, ], label = \"Healthy\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    signal1 = [i for i,j in enumerate(H[0, ]) if j == np.max(H[0, ])][0]\n",
    "    signal2 = [i for i,j in enumerate(H[1, ]) if j == np.max(H[1, ])][0]\n",
    "\n",
    "    if (signal1 < signal2):\n",
    "        nmf_signal_cancer = 1\n",
    "    else:\n",
    "        nmf_signal_cancer = 2\n",
    "    pd.DataFrame(data = [nmf_signal_cancer], columns = [\"nmf_signal_cancer\"]).to_csv(f\"{path_to_model_files}/NMF_{input_feature}_cancer_signal.csv\")\n",
    "    filename = os.path.join(path_to_model_files, f'NMF_{input_feature}.sav')\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "    \n",
    "    tmpdf = nmfdf[[\"SampleID\", f\"V{nmf_signal_cancer}_scale\"]].copy()\n",
    "    tmpdf.columns = [\"SampleID\", f\"NMF_{input_feature}_{nmf_signal_cancer}\"]\n",
    "    train_features[f\"NMF_{input_feature}_{nmf_signal_cancer}\"] = tmpdf.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d094eea7",
   "metadata": {},
   "source": [
    "# Main analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c87e455",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(\n",
    "    train_featuredf[\"IchorCNA\"].merge(metadata, right_on = \"SampleID\", left_on = \"SampleID\"),\n",
    "    x = \"Cancer\",\n",
    "    y = \"ichorCNA\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71f045e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(\n",
    "    test_featuredf[\"IchorCNA\"].merge(metadata, right_on = \"SampleID\", left_on = \"SampleID\"),\n",
    "    x = \"Cancer\",\n",
    "    y = \"ichorCNA\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc8909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(\n",
    "    validate_featuredf[\"IchorCNA\"].merge(metadata, right_on = \"SampleID\", left_on = \"SampleID\"),\n",
    "    x = \"Cancer\",\n",
    "    y = \"ichorCNA\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c1d019",
   "metadata": {},
   "source": [
    "## save sample order and feature order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9023b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in train_featuredf.keys():\n",
    "    train_featuredf[f].to_csv(f\"{path_to_save_feature_order}/train_{f}.csv\", index = False)        \n",
    "    test_featuredf[f].to_csv(f\"{path_to_save_feature_order}/test_{f}.csv\", index = False)        \n",
    "    validate_featuredf[f].to_csv(f\"{path_to_save_feature_order}/validate_{f}.csv\", index = False)        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae740ae7",
   "metadata": {},
   "source": [
    "## Generate cut-off for this analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214e1163",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####--------------------------------------------------------------#####\n",
    "##### Generate cut-off for this release\n",
    "#####--------------------------------------------------------------#####\n",
    "\n",
    "##### absolute difference between a sample and the reference\n",
    "# prepare references\n",
    "ref = dict()\n",
    "\n",
    "em_ref = pd.read_csv(f\"{path_to_model_files}/Healthy_reference_EM.csv\")\n",
    "em_ref.columns = [\"motif\", \"Healthy\"]\n",
    "ref[\"EM\"] = em_ref.copy()\n",
    "\n",
    "flen_ref = pd.read_csv(f\"{path_to_model_files}/Healthy_reference_FLEN.csv\")\n",
    "flen_ref.columns = [\"FLEN\", \"Healthy\"]\n",
    "ref[\"FLEN\"] = flen_ref.copy()\n",
    "\n",
    "nuc_ref = pd.read_csv(f\"{path_to_model_files}/Healthy_reference_NUCLEOSOME.csv\")\n",
    "nuc_ref.columns = [\"Nucleosome\", \"Healthy\"]\n",
    "ref[\"NUCLEOSOME\"] = nuc_ref.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8e8907",
   "metadata": {},
   "source": [
    "## Apply to train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f7e9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples = train_featuredf[\"FLEN\"].SampleID.unique()\n",
    "\n",
    "# add score EM, FLEN, NUCLEOSOME to train_features\n",
    "for f in [\"EM\", \"FLEN\", \"NUCLEOSOME\"]:\n",
    "    inputdf = train_featuredf[f].set_index(\"SampleID\").T.copy()\n",
    "    inputdf[\"Healthy\"] = ref[f][\"Healthy\"].values\n",
    "    for sampleid in all_samples:\n",
    "        inputdf[sampleid] = abs(inputdf[sampleid] - inputdf[\"Healthy\"])\n",
    "    input_scoredf = inputdf.drop(\"Healthy\", axis = 1).sum().reset_index()\n",
    "    input_scoredf.columns = [\"SampleID\", f\"{f}_score\"]\n",
    "    input_scoredf = input_scoredf.merge(metadata, right_on = \"SampleID\", left_on = \"SampleID\")\n",
    "    train_features[f\"{f}_score\"] = input_scoredf\n",
    "\n",
    "# add Shannon entropy for EM feature\n",
    "f = \"EM\"\n",
    "inputdf = train_featuredf[f].set_index(\"SampleID\").T\n",
    "\n",
    "em_shannondf = pd.DataFrame(data = inputdf.columns, columns = [\"SampleID\"])\n",
    "def calculate_em_shannon(x, inputdf):\n",
    "    tmpdf = inputdf[x].values\n",
    "    shannon = -np.sum([item * np.log2(item) for item in tmpdf])/256\n",
    "    return(shannon)\n",
    "em_shannondf[\"EM_shannon\"] = em_shannondf[\"SampleID\"].apply(lambda x: calculate_em_shannon(x, inputdf))\n",
    "em_shannondf = em_shannondf.merge(metadata, right_on = \"SampleID\", left_on = \"SampleID\")\n",
    "train_features[\"EM_shannon\"] = em_shannondf\n",
    "\n",
    "##### OT distance\n",
    "for f in [\"EM\", \"FLEN\", \"NUCLEOSOME\"]:\n",
    "    barycenter = pd.read_csv(f\"{path_to_model_files}/Healthy_OT_{f}_baryl2.csv\")\n",
    "    bary_l2 = barycenter.baryl2.to_numpy()\n",
    "    ot_scoredf = pd.DataFrame(data = all_samples, columns = [\"SampleID\"])\n",
    "    ot_scoredf[f\"OT_{f}\"] = ot_scoredf[\"SampleID\"].apply(lambda x: \n",
    "        calculate_ot_distance_to_healthy_nuc(x, \n",
    "                                             bary_l2, \n",
    "                                             train_featuredf[f].set_index(\"SampleID\").T, \n",
    "                                             n = train_featuredf[f].shape[1] - 1))\n",
    "    ot_scoredf = ot_scoredf.merge(metadata, right_on = \"SampleID\", left_on = \"SampleID\")\n",
    "    train_features[f\"OT_{f}\"] = ot_scoredf\n",
    "    \n",
    "train_features[\"ichorCNA\"] = train_featuredf[\"IchorCNA\"]\n",
    "train_outputdf = pd.DataFrame(data = metadata[\"SampleID\"].to_list(), columns = [\"SampleID\"])\n",
    "for feat in train_features.keys():\n",
    "    tmpdf = train_features[feat][[\"SampleID\", feat]]\n",
    "    tmpdf.columns = [\"SampleID\", feat]\n",
    "    train_outputdf = train_outputdf.merge(tmpdf, right_on = \"SampleID\", left_on = \"SampleID\")\n",
    "\n",
    "train_outputdf = train_outputdf.merge(metadata, right_on = \"SampleID\", left_on = \"SampleID\")\n",
    "\n",
    "\n",
    "\n",
    "train_outputdf[\"True_label\"] = train_outputdf[[\"Cancer\", \"Label\"]].apply(\n",
    "    lambda x: x[1] if x[0] != \"Healthy\" else \"Healthy\", axis = 1\n",
    ")\n",
    "\n",
    "train_outputdf[\"Label\"] = train_outputdf[\"Label\"].apply(lambda x: 1 if x == \"Pos\" else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1eeebb",
   "metadata": {},
   "source": [
    "## generate cut-off "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26ecd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### generate cut-off max spec 100%\n",
    "all_cutoff_df = dict()\n",
    "\n",
    "spec100_cutoffdf = train_outputdf[train_outputdf[\"SampleID\"].isin(train_samplelist[\"Healthy\"])][ [\"SampleID\"] + list(train_features.keys())].set_index(\"SampleID\").max().reset_index()\n",
    "spec100_cutoffdf.columns = [\"feature\", \"cutoff\"]\n",
    "spec100_cutoffdf.to_csv(f\"{path_to_model_files}/cutoff_SPEC100.csv\", index = False)\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "all_cut_off = dict()\n",
    "\n",
    "for spec_thres in [0.99, 0.98, 0.95, 0.90]: \n",
    "    all_cut_off[spec_thres] = dict()\n",
    "\n",
    "    for feat in spec100_cutoffdf.feature.unique():\n",
    "        tmpdf = train_outputdf[[\"SampleID\", feat, \"Label\"]].copy()\n",
    "\n",
    "        fpr, tpr, thresholds = roc_curve(tmpdf[\"Label\"].values, tmpdf[feat].values, pos_label=1)\n",
    "        aucdf = pd.DataFrame({\"fpr\": fpr, \"tpr\": tpr, \"thresholds\": thresholds})\n",
    "        aucdf_spec95 = aucdf[(aucdf[\"fpr\"] <= 1 - spec_thres)]\n",
    "        selected_thres = aucdf_spec95[aucdf_spec95[\"tpr\"] == aucdf_spec95.tpr.max()][\"thresholds\"].values[0]\n",
    "        all_cut_off[spec_thres][feat] = selected_thres\n",
    "\n",
    "    spec_cutoffdf = pd.DataFrame.from_dict(all_cut_off[spec_thres], orient = \"index\").reset_index()\n",
    "    spec_cutoffdf.columns = [\"feature\", \"cutoff\"]\n",
    "\n",
    "    all_cutoff_df[spec_thres] = spec_cutoffdf\n",
    "\n",
    "all_cutoff_df[1] = spec100_cutoffdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bc88d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "cutoffdf = all_cutoff_df[expected_spec]\n",
    "\n",
    "all_features = train_features.keys()\n",
    "for feat in all_features:\n",
    "    c = float(cutoffdf[cutoffdf[\"feature\"] == feat].cutoff.values[0])\n",
    "    train_outputdf[f\"prediction_{feat}\"] = train_outputdf[feat].apply(\n",
    "        lambda x: 1 if x > c else 0\n",
    "    )\n",
    "train_outputdf = train_outputdf[train_outputdf[\"Label\"] != \"?\"]\n",
    "train_resdf = pd.DataFrame(data = all_features, columns = [\"feature\"])\n",
    "train_resdf[\"SEN\"] = train_resdf[\"feature\"].apply(\n",
    "    lambda x: train_outputdf[(train_outputdf[f\"prediction_{x}\"] == 1) & (train_outputdf[\"Label\"] == 1)].shape[0]/train_outputdf[train_outputdf[\"Label\"]== 1].shape[0]\n",
    ")\n",
    "train_resdf[\"SPEC\"] = train_resdf[\"feature\"].apply(\n",
    "    lambda x: train_outputdf[(train_outputdf[f\"prediction_{x}\"] == 0) & (train_outputdf[\"Label\"] == 0)].shape[0]/train_outputdf[train_outputdf[\"Label\"]== 0].shape[0]\n",
    ")\n",
    "feature_combinations = []\n",
    "for i in range(1, len(all_features) + 1):\n",
    "    feature_combinations.extend(combinations(all_features, i))\n",
    "\n",
    "train_combinedf = pd.DataFrame(data = [\",\".join(feature_combinations[i]) for i in range(len(feature_combinations))], columns = [\"feature_combinations\"])\n",
    "def get_Sen_Spec_for_combi(combi, inputdf):\n",
    "    input_feats = combi.split(\",\")\n",
    "    tmpdf =  inputdf[[\"Label\"] + [f\"prediction_{i}\" for i in input_feats]]\n",
    "    tmpdf[\"sum\"] = tmpdf[[f\"prediction_{i}\" for i in input_feats]].sum(axis = 1)\n",
    "    tmpdf[\"prediction\"] = tmpdf[\"sum\"].apply(lambda x: 1 if x != 0 else 0)\n",
    "    sen = tmpdf[(tmpdf[\"prediction\"] == 1) & (tmpdf[\"Label\"] == 1)].shape[0]/tmpdf[tmpdf[\"Label\"]== 1].shape[0]\n",
    "    spec = tmpdf[(tmpdf[\"prediction\"] == 0) & (tmpdf[\"Label\"] == 0)].shape[0]/tmpdf[tmpdf[\"Label\"]== 0].shape[0]\n",
    "    return(sen, spec)\n",
    "\n",
    "train_combinedf[[\"SEN_Train\", \"SPEC_Train\"]] = train_combinedf[\"feature_combinations\"].apply(lambda x: get_Sen_Spec_for_combi(x, train_outputdf)).apply(pd.Series)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42637474",
   "metadata": {},
   "source": [
    "## Cut-off "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6598cc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoffdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeed3933",
   "metadata": {},
   "source": [
    "## Apply to test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dad8d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = dict()\n",
    "\n",
    "all_samples = test_featuredf[\"FLEN\"].SampleID.unique()\n",
    "\n",
    "# add score EM, FLEN, NUCLEOSOME to test_features\n",
    "for f in [\"EM\", \"FLEN\", \"NUCLEOSOME\"]:\n",
    "    inputdf = test_featuredf[f].set_index(\"SampleID\").T.copy()\n",
    "    inputdf[\"Healthy\"] = ref[f][\"Healthy\"].values\n",
    "    for sampleid in all_samples:\n",
    "        inputdf[sampleid] = abs(inputdf[sampleid] - inputdf[\"Healthy\"])\n",
    "    input_scoredf = inputdf.drop(\"Healthy\", axis = 1).sum().reset_index()\n",
    "    input_scoredf.columns = [\"SampleID\", f\"{f}_score\"]\n",
    "    input_scoredf = input_scoredf.merge(metadata, right_on = \"SampleID\", left_on = \"SampleID\")\n",
    "    test_features[f\"{f}_score\"] = input_scoredf\n",
    "\n",
    "f = \"EM\"\n",
    "inputdf = test_featuredf[f].set_index(\"SampleID\").T\n",
    "\n",
    "em_shannondf = pd.DataFrame(data = inputdf.columns, columns = [\"SampleID\"])\n",
    "def calculate_em_shannon(x, inputdf):\n",
    "    tmpdf = inputdf[x].values\n",
    "    shannon = -np.sum([item * np.log2(item) for item in tmpdf])/256\n",
    "    return(shannon)\n",
    "em_shannondf[\"EM_shannon\"] = em_shannondf[\"SampleID\"].apply(lambda x: calculate_em_shannon(x, inputdf))\n",
    "em_shannondf = em_shannondf.merge(metadata, right_on = \"SampleID\", left_on = \"SampleID\")\n",
    "test_features[\"EM_shannon\"] = em_shannondf\n",
    "##### OT distance\n",
    "for f in [\"EM\", \"FLEN\", \"NUCLEOSOME\"]:\n",
    "    barycenter = pd.read_csv(f\"{path_to_model_files}/Healthy_OT_{f}_baryl2.csv\")\n",
    "    bary_l2 = barycenter.baryl2.to_numpy()\n",
    "    ot_scoredf = pd.DataFrame(data = all_samples, columns = [\"SampleID\"])\n",
    "    ot_scoredf[f\"OT_{f}\"] = ot_scoredf[\"SampleID\"].apply(lambda x: \n",
    "        calculate_ot_distance_to_healthy_nuc(x, \n",
    "                                             bary_l2, \n",
    "                                             test_featuredf[f].set_index(\"SampleID\").T, \n",
    "                                             n = test_featuredf[f].shape[1] - 1))\n",
    "    ot_scoredf = ot_scoredf.merge(metadata, right_on = \"SampleID\", left_on = \"SampleID\")\n",
    "    test_features[f\"OT_{f}\"] = ot_scoredf\n",
    "    \n",
    "test_features[\"ichorCNA\"] = test_featuredf[\"IchorCNA\"]\n",
    "\n",
    "for input_feature in [\"EM\", \"FLEN\", \"NUCLEOSOME\"]:\n",
    "    filename = os.path.join(path_to_model_files, f'NMF_{input_feature}.sav')\n",
    "    model = pickle.load(open(filename, 'rb'))\n",
    "    X = test_featuredf[input_feature].set_index(\"SampleID\")\n",
    "    W = model.transform(X.to_numpy())\n",
    "    H = model.components_\n",
    "    nmf_signal_cancer = cutoffdf[cutoffdf[\"feature\"].str.contains(input_feature)][\"feature\"].values[0].split(\"_\")[2]\n",
    "    nmfdf = pd.DataFrame(data = W, columns = [\"V1\", \"V2\"])\n",
    "    nmfdf[\"SampleID\"] = list(X.index)\n",
    "    nmfdf[\"V1_scale\"] = nmfdf[[\"V1\", \"V2\"]].apply(lambda x: x[0]/sum(x), axis = 1)\n",
    "    nmfdf[\"V2_scale\"] = nmfdf[[\"V1\", \"V2\"]].apply(lambda x: x[1]/sum(x), axis = 1)\n",
    "    nmfdf = nmfdf.merge(metadata, right_on = \"SampleID\", left_on = \"SampleID\")\n",
    "    tmpdf = nmfdf[[\"SampleID\", f\"V{nmf_signal_cancer}_scale\"]].copy()\n",
    "    tmpdf.columns = [\"SampleID\", f\"NMF_{input_feature}_{nmf_signal_cancer}\"]\n",
    "    test_features[f\"NMF_{input_feature}_{nmf_signal_cancer}\"] = tmpdf.copy()\n",
    "test_outputdf = pd.DataFrame(data = metadata[\"SampleID\"].to_list(), columns = [\"SampleID\"])\n",
    "for feat in test_features.keys():\n",
    "    tmpdf = test_features[feat][[\"SampleID\", feat]]\n",
    "    tmpdf.columns = [\"SampleID\", feat]\n",
    "    test_outputdf = test_outputdf.merge(tmpdf, right_on = \"SampleID\", left_on = \"SampleID\")\n",
    "\n",
    "test_outputdf = test_outputdf.merge(metadata, right_on = \"SampleID\", left_on = \"SampleID\")\n",
    "\n",
    "test_outputdf[\"True_label\"] = test_outputdf[[\"Cancer\", \"Label\"]].apply(\n",
    "    lambda x: x[1] if x[0] != \"Healthy\" else \"Healthy\", axis = 1\n",
    ")\n",
    "\n",
    "test_outputdf[\"Label\"] = test_outputdf[\"Label\"].apply(lambda x: 1 if x == \"Pos\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03f7b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = test_features.keys()\n",
    "for feat in all_features:\n",
    "    c = float(cutoffdf[cutoffdf[\"feature\"] == feat].cutoff.values[0])\n",
    "    test_outputdf[f\"prediction_{feat}\"] = test_outputdf[feat].apply(\n",
    "        lambda x: 1 if x > c else 0\n",
    "    )\n",
    "test_outputdf = test_outputdf[test_outputdf[\"Label\"] != \"?\"]\n",
    "test_combinedf = pd.DataFrame(data = [\",\".join(feature_combinations[i]) for i in range(len(feature_combinations))], columns = [\"feature_combinations\"])\n",
    "\n",
    "test_combinedf[[\"SEN_test\", \"SPEC_test\"]] = test_combinedf[\"feature_combinations\"].apply(lambda x: get_Sen_Spec_for_combi(x, test_outputdf)).apply(pd.Series)\n",
    "test_combinedf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44e179d",
   "metadata": {},
   "source": [
    "## Apply to validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f456380b",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_features = dict()\n",
    "\n",
    "all_samples = validate_featuredf[\"FLEN\"].SampleID.unique()\n",
    "\n",
    "# add score EM, FLEN, NUCLEOSOME to validate_features\n",
    "for f in [\"EM\", \"FLEN\", \"NUCLEOSOME\"]:\n",
    "    inputdf = validate_featuredf[f].set_index(\"SampleID\").T.copy()\n",
    "    inputdf[\"Healthy\"] = ref[f][\"Healthy\"].values\n",
    "    for sampleid in all_samples:\n",
    "        inputdf[sampleid] = abs(inputdf[sampleid] - inputdf[\"Healthy\"])\n",
    "    input_scoredf = inputdf.drop(\"Healthy\", axis = 1).sum().reset_index()\n",
    "    input_scoredf.columns = [\"SampleID\", f\"{f}_score\"]\n",
    "    input_scoredf = input_scoredf.merge(metadata, right_on = \"SampleID\", left_on = \"SampleID\")\n",
    "    validate_features[f\"{f}_score\"] = input_scoredf\n",
    "\n",
    "f = \"EM\"\n",
    "inputdf = validate_featuredf[f].set_index(\"SampleID\").T\n",
    "\n",
    "em_shannondf = pd.DataFrame(data = inputdf.columns, columns = [\"SampleID\"])\n",
    "def calculate_em_shannon(x, inputdf):\n",
    "    tmpdf = inputdf[x].values\n",
    "    shannon = -np.sum([item * np.log2(item) for item in tmpdf])/256\n",
    "    return(shannon)\n",
    "em_shannondf[\"EM_shannon\"] = em_shannondf[\"SampleID\"].apply(lambda x: calculate_em_shannon(x, inputdf))\n",
    "em_shannondf = em_shannondf.merge(metadata, right_on = \"SampleID\", left_on = \"SampleID\")\n",
    "validate_features[\"EM_shannon\"] = em_shannondf\n",
    "##### OT distance\n",
    "for f in [\"EM\", \"FLEN\", \"NUCLEOSOME\"]:\n",
    "    barycenter = pd.read_csv(f\"{path_to_model_files}/Healthy_OT_{f}_baryl2.csv\")\n",
    "    bary_l2 = barycenter.baryl2.to_numpy()\n",
    "    ot_scoredf = pd.DataFrame(data = all_samples, columns = [\"SampleID\"])\n",
    "    ot_scoredf[f\"OT_{f}\"] = ot_scoredf[\"SampleID\"].apply(lambda x: \n",
    "        calculate_ot_distance_to_healthy_nuc(x, \n",
    "                                             bary_l2, \n",
    "                                             validate_featuredf[f].set_index(\"SampleID\").T, \n",
    "                                             n = validate_featuredf[f].shape[1] - 1))\n",
    "    ot_scoredf = ot_scoredf.merge(metadata, right_on = \"SampleID\", left_on = \"SampleID\")\n",
    "    validate_features[f\"OT_{f}\"] = ot_scoredf\n",
    "    \n",
    "validate_features[\"ichorCNA\"] = validate_featuredf[\"IchorCNA\"]\n",
    "\n",
    "for input_feature in [\"EM\", \"FLEN\", \"NUCLEOSOME\"]:\n",
    "    filename = os.path.join(path_to_model_files, f'NMF_{input_feature}.sav')\n",
    "    model = pickle.load(open(filename, 'rb'))\n",
    "    X = validate_featuredf[input_feature].set_index(\"SampleID\")\n",
    "    W = model.transform(X.to_numpy())\n",
    "    H = model.components_\n",
    "    nmf_signal_cancer = cutoffdf[cutoffdf[\"feature\"].str.contains(input_feature)][\"feature\"].values[0].split(\"_\")[2]\n",
    "    nmfdf = pd.DataFrame(data = W, columns = [\"V1\", \"V2\"])\n",
    "    nmfdf[\"SampleID\"] = list(X.index)\n",
    "    nmfdf[\"V1_scale\"] = nmfdf[[\"V1\", \"V2\"]].apply(lambda x: x[0]/sum(x), axis = 1)\n",
    "    nmfdf[\"V2_scale\"] = nmfdf[[\"V1\", \"V2\"]].apply(lambda x: x[1]/sum(x), axis = 1)\n",
    "    nmfdf = nmfdf.merge(metadata, right_on = \"SampleID\", left_on = \"SampleID\")\n",
    "    tmpdf = nmfdf[[\"SampleID\", f\"V{nmf_signal_cancer}_scale\"]].copy()\n",
    "    tmpdf.columns = [\"SampleID\", f\"NMF_{input_feature}_{nmf_signal_cancer}\"]\n",
    "    validate_features[f\"NMF_{input_feature}_{nmf_signal_cancer}\"] = tmpdf.copy()\n",
    "validate_outputdf = pd.DataFrame(data = metadata[\"SampleID\"].to_list(), columns = [\"SampleID\"])\n",
    "for feat in validate_features.keys():\n",
    "    tmpdf = validate_features[feat][[\"SampleID\", feat]]\n",
    "    tmpdf.columns = [\"SampleID\", feat]\n",
    "    validate_outputdf = validate_outputdf.merge(tmpdf, right_on = \"SampleID\", left_on = \"SampleID\")\n",
    "\n",
    "validate_outputdf = validate_outputdf.merge(metadata, right_on = \"SampleID\", left_on = \"SampleID\")\n",
    "\n",
    "validate_outputdf[\"True_label\"] = validate_outputdf[[\"Cancer\", \"Label\"]].apply(\n",
    "    lambda x: x[1] if x[0] != \"Healthy\" else \"Healthy\", axis = 1\n",
    ")\n",
    "\n",
    "validate_outputdf[\"Label\"] = validate_outputdf[\"Label\"].apply(lambda x: 1 if x == \"Pos\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29245c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = validate_features.keys()\n",
    "for feat in all_features:\n",
    "    c = float(cutoffdf[cutoffdf[\"feature\"] == feat].cutoff.values[0])\n",
    "    validate_outputdf[f\"prediction_{feat}\"] = validate_outputdf[feat].apply(\n",
    "        lambda x: 1 if x > c else 0\n",
    "    )\n",
    "validate_outputdf = validate_outputdf[validate_outputdf[\"Label\"] != \"?\"]\n",
    "validate_combinedf = pd.DataFrame(data = [\",\".join(feature_combinations[i]) for i in range(len(feature_combinations))], columns = [\"feature_combinations\"])\n",
    "\n",
    "validate_combinedf[[\"SEN_validate\", \"SPEC_validate\"]] = validate_combinedf[\"feature_combinations\"].apply(lambda x: get_Sen_Spec_for_combi(x, validate_outputdf)).apply(pd.Series)\n",
    "validate_combinedf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cde59dc",
   "metadata": {},
   "source": [
    "## summary all results in train - test - validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5323c02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarydf = train_combinedf.merge(test_combinedf, right_on = \"feature_combinations\", left_on = \"feature_combinations\")\n",
    "summarydf = summarydf.merge(validate_combinedf, right_on = \"feature_combinations\", left_on = \"feature_combinations\")\n",
    "summarydf[\"num_combi\"] = summarydf[\"feature_combinations\"].apply(lambda x: len(x.split(\",\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c0f94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarydf.sort_values(by = [\"SPEC_Train\", \"SEN_Train\"], ascending = [False, False]).to_excel(f\"{path_to_model_files}/summary.xlsx\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e331ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarydf.sort_values(by = [\"SPEC_Train\", \"SEN_Train\"], ascending = [False, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cfa743",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_combi in summarydf[\"num_combi\"].unique():  \n",
    "    summarydf[summarydf[\"num_combi\"] == num_combi].sort_values(by = [\"SPEC_Train\", \"SEN_Train\"], ascending = [False, False]).to_excel(f\"{path_to_model_files}/summary_{num_combi}combi.xlsx\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e43e77",
   "metadata": {},
   "source": [
    "# Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473bdf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.groupby([\"Label\", \"Set\"])[\"SampleID\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36ec16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.groupby([\"Team run\", \"Set\"])[\"SampleID\"].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64eed95",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_outputdf.to_excel(f\"{path_to_model_files}/train_output.xlsx\", index = False)\n",
    "test_outputdf.to_excel(f\"{path_to_model_files}/test_output.xlsx\", index = False)\n",
    "validate_outputdf.to_excel(f\"{path_to_model_files}/validate_output.xlsx\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598ee1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_outputdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5714dd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_outputdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155f6970",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_outputdf.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mrd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
