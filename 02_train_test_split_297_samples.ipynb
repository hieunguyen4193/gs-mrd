{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.decomposition import NMF\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import os\n",
    "import ot\n",
    "import pickle\n",
    "import argparse\n",
    "import Levenshtein\n",
    "import itertools\n",
    "from helper_functions import *\n",
    "##### input args\n",
    "PROJECT = \"gs-mrd\"\n",
    "release_version = \"09122024_297_samples\"\n",
    "merge_version = \"20240914\"\n",
    "\n",
    "##### configurations/paths\n",
    "path_to_main_src = \"/media/hieunguyen/HNSD01/src/gs-mrd\"\n",
    "path_to_merge_samples = f\"{path_to_main_src}/all_samples/{merge_version}\"\n",
    "\n",
    "path_to_save_output = os.path.join(path_to_main_src, \"output\")\n",
    "path_to_02_output = os.path.join(path_to_save_output, \"02_output\")\n",
    "\n",
    "path_to_model_files = f\"{path_to_02_output}/model_files/{release_version}\"\n",
    "path_to_save_features = f\"{path_to_02_output}/model_files/{release_version}/features\"\n",
    "\n",
    "os.system(f\"mkdir -p {path_to_save_features}\")\n",
    "os.system(f\"mkdir -p {path_to_02_output}\")\n",
    "\n",
    "#####----------------------------------------------------------------------#####\n",
    "##### PREPARE METADATA\n",
    "#####----------------------------------------------------------------------#####\n",
    "metadata = pd.read_csv(\"/media/hieunguyen/HNSD01/src/gs-mrd/model_files/10062024/release_metadata.csv\")\n",
    "\n",
    "general_metadata = pd.read_excel(\"All Samples GW_MRD_010924.modified.xlsx\", index_col = [0])\n",
    "\n",
    "metadata[\"Cancer\"] = metadata[\"SampleID\"].apply(lambda x: general_metadata[general_metadata[\"SampleID\"] == x.split(\"-\")[1]].Cancer.unique()[0])\n",
    "metadata[\"True label\"] = metadata[\"SampleID\"].apply(lambda x: general_metadata[general_metadata[\"SampleID\"] == x.split(\"-\")[1]][\"True label\"].unique()[0])\n",
    "metadata[\"SampleID2\"] = metadata[\"SampleID\"].apply(lambda x: x.split(\"-\")[1])\n",
    "motif_order = pd.read_csv(\"motif_order.csv\").motif_order.to_list()\n",
    "metadata297 = pd.read_csv(\"train_test_split_297_samples.csv\")\n",
    "metadata[\"Set\"] = metadata[\"SampleID2\"].apply(\n",
    "    lambda x: metadata297[metadata297[\"SampleID\"] == x][\"Set\"].unique()[0] if x in metadata297[\"SampleID\"].to_list() else \"validation\"\n",
    "    )\n",
    "metadata = metadata[metadata[\"True label\"] != \"?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####----------------------------------------------------------------------#####\n",
    "##### PREPARE FEATURES\n",
    "#####----------------------------------------------------------------------#####\n",
    "samplelist = dict()\n",
    "train_samplelist = dict()\n",
    "test_samplelist = dict()\n",
    "\n",
    "all_train_samples = []\n",
    "all_test_samples = []\n",
    "for label in metadata.Cancer.unique():\n",
    "    samplelist[label] = metadata[metadata[\"Cancer\"] == label][\"SampleID\"].to_list()\n",
    "    train_samplelist[label] = metadata[(metadata[\"Set\"] == \"train\") & (metadata[\"Cancer\"] == label)][\"SampleID\"].to_list()\n",
    "    test_samplelist[label] = metadata[(metadata[\"Set\"] == \"validation\") & (metadata[\"Cancer\"] == label)][\"SampleID\"].to_list()\n",
    "    all_train_samples = all_train_samples + train_samplelist[label]\n",
    "    all_test_samples = all_test_samples + test_samplelist[label]\n",
    "\n",
    "full_featuredf = dict()\n",
    "train_featuredf = dict()\n",
    "test_featuredf = dict()\n",
    "for input_feature in [\"EM\", \"FLEN\", \"NUCLEOSOME\", \"IchorCNA\"]:\n",
    "    tmpdf = pd.read_csv(f\"{path_to_merge_samples}/{input_feature}_features.csv\")\n",
    "    tmpdf = tmpdf[tmpdf[\"SampleID\"].isin(metadata.SampleID.unique())]\n",
    "    if input_feature == \"EM\":\n",
    "        full_featuredf[input_feature] = tmpdf[[\"SampleID\"] + motif_order].copy()\n",
    "    else:\n",
    "        full_featuredf[input_feature] = tmpdf.copy()\n",
    "        \n",
    "    assert full_featuredf[input_feature].shape[0] == metadata.shape[0]\n",
    "    full_featuredf[input_feature].to_csv(f\"{path_to_save_features}/{input_feature}_full_features.csv\", index = False)\n",
    "    train_featuredf[input_feature] = tmpdf[tmpdf[\"SampleID\"].isin(all_train_samples)]\n",
    "    test_featuredf[input_feature] = tmpdf[tmpdf[\"SampleID\"].isin(all_test_samples)]\n",
    "    train_featuredf[input_feature].to_csv(f\"{path_to_save_features}/{input_feature}_train_features.csv\", index = False)\n",
    "    test_featuredf[input_feature].to_csv(f\"{path_to_save_features}/{input_feature}_test_features.csv\", index = False)\n",
    "    \n",
    "##### distance matrix based on edit distance of End motif 4bp\n",
    "nucleotides = ['A', 'C', 'G', 'T']\n",
    "motifs = [''.join(p) for p in itertools.product(nucleotides, repeat=4)]\n",
    "\n",
    "# Initialize an empty distance matrix\n",
    "distance_matrix = pd.DataFrame(index=motifs, columns=motifs)\n",
    "\n",
    "# Compute the Levenshtein distance between each pair of 4-mer motifs\n",
    "for motif1 in motifs:\n",
    "    for motif2 in motifs:\n",
    "        distance_matrix.loc[motif1, motif2] = Levenshtein.distance(motif1, motif2)\n",
    "\n",
    "# Convert the distance matrix to integer type\n",
    "M_EM = distance_matrix.to_numpy().copy()\n",
    "M_EM /= M_EM.max() * 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the NMF and OT distance barycenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = dict() \n",
    "\n",
    "for input_feature in [\"EM\", \"FLEN\", \"NUCLEOSOME\"]:\n",
    "    ##### generate average FEATURE in all control samples in this batch\n",
    "    inputdf = train_featuredf[input_feature].copy().set_index(\"SampleID\").T\n",
    "    inputdf[\"Healthy\"] = inputdf[train_samplelist[\"Healthy\"]].mean(axis = 1)\n",
    "    inputdf[[\"Healthy\"]].to_csv(f\"{path_to_model_files}/Healthy_reference_{input_feature}.csv\")\n",
    "    inputdf = inputdf.drop(\"Healthy\", axis = 1)\n",
    "    \n",
    "    ##### calculate OT barycenters\n",
    "    if input_feature == \"EM\":\n",
    "        baryl2 = calculate_barycenter(inputdf = train_featuredf[input_feature].set_index(\"SampleID\").T,\n",
    "                                      samplelist = train_samplelist, \n",
    "                                      n = inputdf.shape[0], show_plot=False, M = M_EM)\n",
    "    else: \n",
    "        baryl2 = calculate_barycenter(inputdf = train_featuredf[input_feature].set_index(\"SampleID\").T,\n",
    "                                      samplelist = train_samplelist, \n",
    "                                      n = inputdf.shape[0], show_plot=False, M = None)\n",
    "    pd.DataFrame(data = baryl2, columns = [\"baryl2\"]).to_csv(f\"{path_to_model_files}/Healthy_OT_{input_feature}_baryl2.csv\", index = False)\n",
    "    \n",
    "    ##### NMF models\n",
    "    X = train_featuredf[input_feature].set_index(\"SampleID\")\n",
    "    model = NMF(n_components=2, init='random', random_state=0, solver = \"mu\")\n",
    "    W = model.fit_transform(X.to_numpy())\n",
    "    H = model.components_\n",
    "    nmfdf = pd.DataFrame(data = W, columns = [\"V1\", \"V2\"])\n",
    "    nmfdf[\"SampleID\"] = list(X.index)\n",
    "    nmfdf[\"V1_scale\"] = nmfdf[[\"V1\", \"V2\"]].apply(lambda x: x[0]/sum(x), axis = 1)\n",
    "    nmfdf[\"V2_scale\"] = nmfdf[[\"V1\", \"V2\"]].apply(lambda x: x[1]/sum(x), axis = 1)\n",
    "    nmfdf = nmfdf.merge(metadata, right_on = \"SampleID\", left_on = \"SampleID\")\n",
    "    sns.lineplot(H[0, ], label = \"Cancer\")\n",
    "    sns.lineplot(H[1, ], label = \"Healthy\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    signal1 = [i for i,j in enumerate(H[0, ]) if j == np.max(H[0, ])][0]\n",
    "    signal2 = [i for i,j in enumerate(H[1, ]) if j == np.max(H[1, ])][0]\n",
    "\n",
    "    if (signal1 < signal2):\n",
    "        nmf_signal_cancer = 1\n",
    "    else:\n",
    "        nmf_signal_cancer = 2\n",
    "    pd.DataFrame(data = [nmf_signal_cancer], columns = [\"nmf_signal_cancer\"]).to_csv(f\"{path_to_model_files}/NMF_{input_feature}_cancer_signal.csv\")\n",
    "    filename = os.path.join(path_to_model_files, f'NMF_{input_feature}.sav')\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "    \n",
    "    tmpdf = nmfdf[[\"SampleID\", f\"V{nmf_signal_cancer}_scale\"]].copy()\n",
    "    tmpdf.columns = [\"SampleID\", f\"NMF_{input_feature}\"]\n",
    "    train_features[f\"NMF_{input_feature}\"] = tmpdf.copy()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform the training data and determine the cut-off, cut-off = maximum value in healthy samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####--------------------------------------------------------------#####\n",
    "##### Generate cut-off for this release\n",
    "#####--------------------------------------------------------------#####\n",
    "\n",
    "##### absolute difference between a sample and the reference\n",
    "# prepare references\n",
    "ref = dict()\n",
    "\n",
    "em_ref = pd.read_csv(f\"{path_to_model_files}/Healthy_reference_EM.csv\")\n",
    "em_ref.columns = [\"motif\", \"Healthy\"]\n",
    "ref[\"EM\"] = em_ref.copy()\n",
    "\n",
    "flen_ref = pd.read_csv(f\"{path_to_model_files}/Healthy_reference_FLEN.csv\")\n",
    "flen_ref.columns = [\"FLEN\", \"Healthy\"]\n",
    "ref[\"FLEN\"] = flen_ref.copy()\n",
    "\n",
    "nuc_ref = pd.read_csv(f\"{path_to_model_files}/Healthy_reference_NUCLEOSOME.csv\")\n",
    "nuc_ref.columns = [\"Nucleosome\", \"Healthy\"]\n",
    "ref[\"NUCLEOSOME\"] = nuc_ref.copy()\n",
    "\n",
    "all_samples = train_featuredf[\"FLEN\"].SampleID.unique()\n",
    "\n",
    "# add score EM, FLEN, NUCLEOSOME to train_features\n",
    "for f in [\"EM\", \"FLEN\", \"NUCLEOSOME\"]:\n",
    "    inputdf = train_featuredf[f].set_index(\"SampleID\").T.copy()\n",
    "    inputdf[\"Healthy\"] = ref[f][\"Healthy\"].values\n",
    "    for sampleid in all_samples:\n",
    "        inputdf[sampleid] = abs(inputdf[sampleid] - inputdf[\"Healthy\"])\n",
    "    input_scoredf = inputdf.drop(\"Healthy\", axis = 1).sum().reset_index()\n",
    "    input_scoredf.columns = [\"SampleID\", f\"{f}_score\"]\n",
    "    input_scoredf = input_scoredf.merge(metadata, right_on = \"SampleID\", left_on = \"SampleID\")\n",
    "    train_features[f\"{f}_score\"] = input_scoredf\n",
    "\n",
    "f = \"EM\"\n",
    "inputdf = train_featuredf[f].set_index(\"SampleID\").T\n",
    "\n",
    "em_shannondf = pd.DataFrame(data = inputdf.columns, columns = [\"SampleID\"])\n",
    "def calculate_em_shannon(x, inputdf):\n",
    "    tmpdf = inputdf[x].values\n",
    "    shannon = -np.sum([item * np.log2(item) for item in tmpdf])/256\n",
    "    return(shannon)\n",
    "em_shannondf[\"EM_shannon\"] = em_shannondf[\"SampleID\"].apply(lambda x: calculate_em_shannon(x, inputdf))\n",
    "em_shannondf = em_shannondf.merge(metadata, right_on = \"SampleID\", left_on = \"SampleID\")\n",
    "train_features[\"EM_shannon\"] = em_shannondf\n",
    "\n",
    "##### OT distance\n",
    "for f in [\"EM\", \"FLEN\", \"NUCLEOSOME\"]:\n",
    "    barycenter = pd.read_csv(f\"{path_to_model_files}/Healthy_OT_{f}_baryl2.csv\")\n",
    "    bary_l2 = barycenter.baryl2.to_numpy()\n",
    "    ot_scoredf = pd.DataFrame(data = all_samples, columns = [\"SampleID\"])\n",
    "    ot_scoredf[f\"OT_{f}\"] = ot_scoredf[\"SampleID\"].apply(lambda x: \n",
    "        calculate_ot_distance_to_healthy_nuc(x, \n",
    "                                             bary_l2, \n",
    "                                             train_featuredf[f].set_index(\"SampleID\").T, \n",
    "                                             n = train_featuredf[f].shape[1] - 1))\n",
    "    ot_scoredf = ot_scoredf.merge(metadata, right_on = \"SampleID\", left_on = \"SampleID\")\n",
    "    train_features[f\"OT_{f}\"] = ot_scoredf\n",
    "    \n",
    "train_features[\"ichorCNA\"] = train_featuredf[\"IchorCNA\"]\n",
    "outputdf = pd.DataFrame(data = metadata[\"SampleID\"].to_list(), columns = [\"SampleID\"])\n",
    "for feat in train_features.keys():\n",
    "    tmpdf = train_features[feat][[\"SampleID\", feat]]\n",
    "    tmpdf.columns = [\"SampleID\", feat]\n",
    "    outputdf = outputdf.merge(tmpdf, right_on = \"SampleID\", left_on = \"SampleID\")\n",
    "\n",
    "outputdf = outputdf.merge(metadata, right_on = \"SampleID\", left_on = \"SampleID\")\n",
    "\n",
    "excluded_samples = [\"20-KABH63\"]\n",
    "cutoffdf = outputdf[outputdf[\"SampleID\"].isin([item for item in samplelist[\"Healthy\"] if item not in excluded_samples])][ [\"SampleID\"] + list(train_features.keys())].set_index(\"SampleID\").max().reset_index()\n",
    "cutoffdf.columns = [\"feature\", \"cutoff\"]\n",
    "cutoffdf.to_csv(f\"{path_to_model_files}/cutoff.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = dict()\n",
    "\n",
    "##### absolute difference between a sample and the reference\n",
    "# prepare references\n",
    "ref = dict()\n",
    "\n",
    "em_ref = pd.read_csv(f\"{path_to_model_files}/Healthy_reference_EM.csv\")\n",
    "em_ref.columns = [\"motif\", \"Healthy\"]\n",
    "ref[\"EM\"] = em_ref.copy()\n",
    "\n",
    "flen_ref = pd.read_csv(f\"{path_to_model_files}/Healthy_reference_FLEN.csv\")\n",
    "flen_ref.columns = [\"FLEN\", \"Healthy\"]\n",
    "ref[\"FLEN\"] = flen_ref.copy()\n",
    "\n",
    "nuc_ref = pd.read_csv(f\"{path_to_model_files}/Healthy_reference_NUCLEOSOME.csv\")\n",
    "nuc_ref.columns = [\"Nucleosome\", \"Healthy\"]\n",
    "ref[\"NUCLEOSOME\"] = nuc_ref.copy()\n",
    "\n",
    "all_samples = test_featuredf[\"FLEN\"].SampleID.unique()\n",
    "\n",
    "# add score EM, FLEN, NUCLEOSOME to test_features\n",
    "for f in [\"EM\", \"FLEN\", \"NUCLEOSOME\"]:\n",
    "    inputdf = test_featuredf[f].set_index(\"SampleID\").T.copy()\n",
    "    inputdf[\"Healthy\"] = ref[f][\"Healthy\"].values\n",
    "    for sampleid in all_samples:\n",
    "        inputdf[sampleid] = abs(inputdf[sampleid] - inputdf[\"Healthy\"])\n",
    "    input_scoredf = inputdf.drop(\"Healthy\", axis = 1).sum().reset_index()\n",
    "    input_scoredf.columns = [\"SampleID\", f\"{f}_score\"]\n",
    "    input_scoredf = input_scoredf.merge(metadata, right_on = \"SampleID\", left_on = \"SampleID\")\n",
    "    test_features[f\"{f}_score\"] = input_scoredf\n",
    "\n",
    "f = \"EM\"\n",
    "inputdf = test_featuredf[f].set_index(\"SampleID\").T\n",
    "\n",
    "em_shannondf = pd.DataFrame(data = inputdf.columns, columns = [\"SampleID\"])\n",
    "def calculate_em_shannon(x, inputdf):\n",
    "    tmpdf = inputdf[x].values\n",
    "    shannon = -np.sum([item * np.log2(item) for item in tmpdf])/256\n",
    "    return(shannon)\n",
    "em_shannondf[\"EM_shannon\"] = em_shannondf[\"SampleID\"].apply(lambda x: calculate_em_shannon(x, inputdf))\n",
    "em_shannondf = em_shannondf.merge(metadata, right_on = \"SampleID\", left_on = \"SampleID\")\n",
    "test_features[\"EM_shannon\"] = em_shannondf\n",
    "##### OT distance\n",
    "for f in [\"EM\", \"FLEN\", \"NUCLEOSOME\"]:\n",
    "    barycenter = pd.read_csv(f\"{path_to_model_files}/Healthy_OT_{f}_baryl2.csv\")\n",
    "    bary_l2 = barycenter.baryl2.to_numpy()\n",
    "    ot_scoredf = pd.DataFrame(data = all_samples, columns = [\"SampleID\"])\n",
    "    ot_scoredf[f\"OT_{f}\"] = ot_scoredf[\"SampleID\"].apply(lambda x: \n",
    "        calculate_ot_distance_to_healthy_nuc(x, \n",
    "                                             bary_l2, \n",
    "                                             test_featuredf[f].set_index(\"SampleID\").T, \n",
    "                                             n = test_featuredf[f].shape[1] - 1))\n",
    "    ot_scoredf = ot_scoredf.merge(metadata, right_on = \"SampleID\", left_on = \"SampleID\")\n",
    "    test_features[f\"OT_{f}\"] = ot_scoredf\n",
    "    \n",
    "test_features[\"ichorCNA\"] = test_featuredf[\"IchorCNA\"]\n",
    "\n",
    "for input_feature in [\"EM\", \"FLEN\", \"NUCLEOSOME\"]:\n",
    "    filename = os.path.join(path_to_model_files, f'NMF_{input_feature}.sav')\n",
    "    model = pickle.load(open(filename, 'rb'))\n",
    "    X = test_featuredf[input_feature].set_index(\"SampleID\")\n",
    "    W = model.transform(X.to_numpy())\n",
    "    H = model.components_\n",
    "    nmf_signal_cancer = cutoffdf[cutoffdf[\"feature\"].str.contains(input_feature)][\"feature\"].values[0].split(\"_\")[2]\n",
    "    nmfdf = pd.DataFrame(data = W, columns = [\"V1\", \"V2\"])\n",
    "    nmfdf[\"SampleID\"] = list(X.index)\n",
    "    nmfdf[\"V1_scale\"] = nmfdf[[\"V1\", \"V2\"]].apply(lambda x: x[0]/sum(x), axis = 1)\n",
    "    nmfdf[\"V2_scale\"] = nmfdf[[\"V1\", \"V2\"]].apply(lambda x: x[1]/sum(x), axis = 1)\n",
    "    nmfdf = nmfdf.merge(metadata, right_on = \"SampleID\", left_on = \"SampleID\")\n",
    "    tmpdf = nmfdf[[\"SampleID\", f\"V{nmf_signal_cancer}_scale\"]].copy()\n",
    "    tmpdf.columns = [\"SampleID\", f\"NMF_{input_feature}\"]\n",
    "    test_features[f\"NMF_{input_feature}\"] = tmpdf.copy()\n",
    "test_outputdf = pd.DataFrame(data = metadata[\"SampleID\"].to_list(), columns = [\"SampleID\"])\n",
    "for feat in test_features.keys():\n",
    "    tmpdf = test_features[feat][[\"SampleID\", feat]]\n",
    "    tmpdf.columns = [\"SampleID\", feat]\n",
    "    test_outputdf = test_outputdf.merge(tmpdf, right_on = \"SampleID\", left_on = \"SampleID\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "test_outputdf = test_outputdf.merge(metadata, right_on = \"SampleID\", left_on = \"SampleID\")\n",
    "all_features = test_features.keys()\n",
    "for feat in all_features:\n",
    "    c = cutoffdf[cutoffdf[\"feature\"] == feat].cutoff.values[0]\n",
    "    test_outputdf[f\"prediction_{feat}\"] = test_outputdf[feat].apply(\n",
    "        lambda x: 1 if x > c else 0\n",
    "    )\n",
    "test_outputdf = test_outputdf[test_outputdf[\"True label\"] != \"?\"]\n",
    "test_outputdf[\"True_label\"] = test_outputdf[\"True label\"].apply(lambda x: 1 if x == \"+\" else 0)\n",
    "# test_resdf = pd.DataFrame(data = all_features, columns = [\"feature\"])\n",
    "# test_resdf[\"SEN\"] = test_resdf[\"feature\"].apply(\n",
    "#     lambda x: test_outputdf[(test_outputdf[f\"prediction_{x}\"] == 1) & (test_outputdf[\"True_label\"] == 1)].shape[0]/test_outputdf[test_outputdf[\"True_label\"]== 1].shape[0]\n",
    "# )\n",
    "# test_resdf[\"SPEC\"] = test_resdf[\"feature\"].apply(\n",
    "#     lambda x: test_outputdf[(test_outputdf[f\"prediction_{x}\"] == 0) & (test_outputdf[\"True_label\"] == 0)].shape[0]/test_outputdf[test_outputdf[\"True_label\"]== 0].shape[0]\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_features = test_features.keys()\n",
    "for feat in all_features:\n",
    "    c = cutoffdf[cutoffdf[\"feature\"] == feat].cutoff.values[0]\n",
    "    outputdf[f\"prediction_{feat}\"] = outputdf[feat].apply(\n",
    "        lambda x: 1 if x > c else 0\n",
    "    )\n",
    "outputdf = outputdf[outputdf[\"True label\"] != \"?\"]\n",
    "outputdf[\"True_label\"] = outputdf[\"True label\"].apply(lambda x: 1 if x == \"+\" else 0)\n",
    "train_resdf = pd.DataFrame(data = all_features, columns = [\"feature\"])\n",
    "train_resdf[\"SEN\"] = train_resdf[\"feature\"].apply(\n",
    "    lambda x: outputdf[(outputdf[f\"prediction_{x}\"] == 1) & (outputdf[\"True_label\"] == 1)].shape[0]/outputdf[outputdf[\"True_label\"]== 1].shape[0]\n",
    ")\n",
    "train_resdf[\"SPEC\"] = train_resdf[\"feature\"].apply(\n",
    "    lambda x: outputdf[(outputdf[f\"prediction_{x}\"] == 0) & (outputdf[\"True_label\"] == 0)].shape[0]/outputdf[outputdf[\"True_label\"]== 0].shape[0]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_combinations = []\n",
    "for i in range(1, len(all_features) + 1):\n",
    "    feature_combinations.extend(combinations(all_features, i))\n",
    "\n",
    "combinedf = pd.DataFrame(data = [\",\".join(feature_combinations[i]) for i in range(len(feature_combinations))], columns = [\"feature_combinations\"])\n",
    "def get_Sen_Spec_for_combi(combi, inputdf):\n",
    "    input_feats = combi.split(\",\")\n",
    "    tmpdf =  inputdf[[\"True_label\"] + [f\"prediction_{i}\" for i in input_feats]]\n",
    "    tmpdf[\"sum\"] = tmpdf[[f\"prediction_{i}\" for i in input_feats]].sum(axis = 1)\n",
    "    tmpdf[\"prediction\"] = tmpdf[\"sum\"].apply(lambda x: 1 if x != 0 else 0)\n",
    "    sen = tmpdf[(tmpdf[\"prediction\"] == 1) & (tmpdf[\"True_label\"] == 1)].shape[0]/tmpdf[tmpdf[\"True_label\"]== 1].shape[0]\n",
    "    spec = tmpdf[(tmpdf[\"prediction\"] == 0) & (tmpdf[\"True_label\"] == 0)].shape[0]/tmpdf[tmpdf[\"True_label\"]== 0].shape[0]\n",
    "    return(sen, spec)\n",
    "\n",
    "combinedf[[\"SEN_Train\", \"SPEC_Train\"]] = combinedf[\"feature_combinations\"].apply(lambda x: get_Sen_Spec_for_combi(x, outputdf)).apply(pd.Series)\n",
    "combinedf[[\"SEN_Validation\", \"SPEC_Validation\"]] = combinedf[\"feature_combinations\"].apply(lambda x: get_Sen_Spec_for_combi(x, test_outputdf)).apply(pd.Series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand the column feature_combinations to one-hot encoding columns\n",
    "for feature in all_features:\n",
    "    combinedf[feature] = combinedf[\"feature_combinations\"].apply(lambda x: 1 if feature in x.split(\",\") else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_outputdf.to_csv(os.path.join(path_to_model_files, \"testdf.csv\"))\n",
    "outputdf.to_csv(os.path.join(path_to_model_files, \"traindf.csv\"))\n",
    "combinedf.to_csv(os.path.join(path_to_model_files, \"combinedf.csv\"))\n",
    "metadata.to_csv(os.path.join(path_to_model_files, \"metadata.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mrd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
